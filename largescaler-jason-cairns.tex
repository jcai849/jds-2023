\documentclass[letterpaper, inpress]{jds} % use this for production

% \documentclass[a4paper, review]{jds}      % use this for review


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% the following edits should be done by Journal typesetters
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\setcounter{page}{1}            % set the first page number
\jdsmonth{July}                 % month
\jdsyear{2023}                  % year
\jdsvolume{xx}                  % volume number
\jdsissue{xx}                   % issue number
\jdsdoi{xx.xxxx/xxxxxxxxx}      % doi
\jdsreceived{July, 2023}       % optional: comment it out if no received date
\jdsaccepted{August, 2023}         % optional: comment it out if no accepted date
%% manually set running header for a shorter list of authors if needed
% \shortauthors{A Author A, et al.}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% edits by authors are given below
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{fancyvrb}
\usepackage{float}
\usepackage[x11names, svgnames, rgb]{xcolor}
\usepackage[utf8]{inputenc}
\usepackage{tikz}
\usetikzlibrary{snakes,arrows,shapes}
\usepackage{amsfonts,amsmath,amssymb,amsthm}
\usepackage{booktabs}

\title{A Platform for Large Scale Statistical Modelling in \proglang{R}}

\author[1]{Jason Cairns\thanks{corresponding email:\href{mailto:jason.cairns@auckland.ac.nz}{jason.cairns@auckland.ac.nz}}}
\author[1]{Simon Urbanek}
\author[1]{Paul Murrell}
\affil[1]{Department of Statistics, University of Auckland, New Zealand}

\begin{document}

\maketitle

\begin{abstract}
With the growing scale of big datasets, fitting novel statistical models on larger-than-memory datasets becomes correspondingly challenging.
This document outlines the development and use of \pkg{largescaler}, a platform developed specifically for the development of statistical models for big datasets.
\end{abstract}
\begin{keywords} % alphabetical; excluding anything in the title already
Big Data;
Distributed Computing;
Modelling;
\end{keywords}

\section{Introduction}%
\label{sec:intro}

The rate of growth of datasets continues to outpace attempts to engage meaningfully with them, as individual computer memory limits are increasingly exceeded~\citep{kleppmann2017dataintensive}.
At the scale of big data, speed also becomes a constraining factor, with concurrency and parallelism being of increasing importance.
The aim of a statistician seeking to gain novel insight from such datasets commonly includes the interactive use of a complex statistical model, often implemented from scratch using \proglang{R}.
No single system satisfactorily provides the capacity to meet this demand.

Those systems that do come close to meeting the demand provide direction regarding how to gain insight from larger-than-memory datasets.
Most importantly, the standard solution for handling big data is to operate over a distributed system~\citep{boja2012distributed}.
Several systems have seen widespread use within the context of data and machine learning pipelines, such as \pkg{Spark}~\citep{zaharia2016apache} and \pkg{Hadoop Map-Reduce}~\citep{shvachko2010hadoop}.
For the statistician mostly familiar with \proglang{R}, these systems provide API's to \proglang{R} where distributed data may be manipulated and pre-made models fitted.
However, these API's are often found lacking when attempted to be used for the creation of complex statistical models that don't come pre-packaged, due to this not being their primary use-case, and \proglang{R} not being their target language.

For instance, \pkg{sparklyr} is an interface to Spark from within R~\citep{luraschi20}.
The user connects to spark and accumulates instructions for the manipulation of a \pkg{Spark} DataFrame object using \pkg{dplyr} commands, then executing the request on the \pkg{Spark} cluster.
It works fluidly when intentions meet the \pkg{Spark} paradigm, but due to it being only an interface to an external system, it is commonly required to program directly to \pkg{Spark} specifications, using \proglang{Scala}, when more complex and custom analyses are required.

The most notable general interface in \proglang{R} that has capabilities for large data is the the \pkg{foreach} package~\citep{microsoft20}.
The backends are provided by other packages, typically named with some form of ``Do\emph{X}'', and can be drivers to external or \proglang{R}-specific systems.
Parallelisation is enabled by some backends, with \pkg{doParallel} allowing parallel computations~\citep{corporation19}, \pkg{doSNOW} enabling cluster access through the \pkg{SNOW} package~\citep{dosnow19}, and \pkg{doMPI} allowing for direct MPI access~\citep{weston17}.
\pkg{foreach} remains an exceptional and general interface, but can only be as performant as the backend driver packages.
The necessity of persisting large data objects and allowing for complex manipulation of them is not met by any of the reviewed package backends.

The most complete self-contained interface that was developed specifically for statistical modelling in \proglang{R} is given by the \pkg{pbdR} collection.
This collection allows for for distributed computing with \proglang{R}~\citep{pbdBASEpackage}, with the name being the abbreviation of Programming with Big Data in R.
The packages include high-performance communication and computation capabilities, including \pkg{RPC}, \pkg{ZeroMQ}, and \pkg{MPI} interfaces.
The collection is extensive, offering several packages for each of the main categories of application functionality, communication, computation, development, I/O, and profiling.
While incredibly performant and great care has been taken to provide an interface as close to native \proglang{R} as possible, the provided packages still require an MPI form of shared program amongst all cluster nodes, which presents an unfamiliar form of program specification to most \proglang{R} users.
In addition, more complex analyses requiring lower-level data manipulation end up increasingly involved with the complexities of MPI, which was authored as a far more general system than its use-case in \pkg{pbdR}, thereby possessing a far greater complexity than necessitated.
Regardless, the direction provided of distributing large data across compute nodes, as a means of statistical interaction, is a concept that has been proven well by this package collection.

Within the motivating context provided, the \pkg{largescaler} project has sought to provide a full stack for working with larger-than-memory data in \proglang{R}, allowing the developer to manipulate distributed data and create arbitrarily complex iterative models with which to fit to the data, over a self-contained user-specified computing cluster. Further details on \pkg{largescaler} are provided in~\citet{cairns2023}.

\section{Construction of a Large-Data Statistical System API for R}

The construction of such \pkg{largescaler} has roots in a theoretical reconsideration of precisely what determines the necessary components of a system capable of complex statistical modelling with larger-than-memory data in a distributed fashion.
The structure of such considerations have been defined principally by the response to linguistic challenges facing such an API.

In order to perform calculations on larger-than-memory data, we need some means of \textbf{representing} the data, in order for it to be tangible and useful.
All objects, regular or irregular, are facilitated in their access and manipulation by way of an \textbf{object system}, which is an organised manner of interaction with objects.
The central constraint is that the data is larger than computer memory.
Data must therefore be split into pieces in order to fit into memory.
Such a structure, common among big data systems, is known as a shard, or a \textbf{chunk}. 
The chunk thus serves as the lowest level object manipulable by the user, and defines the lower level of the layered object system.
The end-user is typically not interested in the chunks making up the object.
Therefore higher-level objects are introduced as compositions of chunks, serving as abstract collections.
Such higher-level objects are termed \textbf{distributed objects}, and serve as the primary data structure interacted with by the high-level user, with a representative example depicted in Figure~\ref{fig:distobj}.

\begin{figure}[ht]
\begin{center}
    \input{distobjref.tex}
\caption{Distributed object, showing chunks and their references across disparate nodes.}
\label{fig:distobj}
\end{center}
\end{figure}

With the establishment of an object system, attention may then be turned to the actualities of \textbf{computation}. 
For each chunk, each operation must possess the following properties:

\begin{itemize}
    \item Operations on chunks must have some means of access to all of the relevant chunk arguments to operate on together;
    \item The result of the operation must be stored somewhere.
\end{itemize}

A \textbf{chunk function applicator} is required that takes a function and some chunks, sending the operation to a node which applies the function to the specified chunks.
The applicator requires some means of access to the respective chunks.
An equivalent \textbf{distributed object function applicator} is to be defined in a similar manner, though for distributed objects.
Likewise, following the conclusion of the operation, some means of access to the result is required - the result is to be kept as a distributed object itself, allowing for further iterative action on it.

Some of the concerns relevent to the applicators include questions of argument sourcing and interaction.
With regards to sourcing, the optimal node should be chosen to perform the operation in order to minimise data movement.
All chunks not present in the operating node process are needed to be made available in order for the operation to take place.
Interaction between chunks, especially in the context of the distributed object function applicator, requires some definition of how these distributed arguments are to be treated in multiple.
For instance, whether to recycle chunks, and how they are to be appropriately combined.

After laying out the distributed objects and the operations that they may be engaged in, we reach the limits of the construction of the system.
The mechanical aspects of the system developed, focus is turned to the more complex and precise notions of the arrangement of these components;
let us expand the construction limits now by exploring and describing the component of time as it relates to the system - specifically, \textbf{concurrency} within the system.
Here, we treat concurrency in the manner of \citet{pike2012concurrency}, where it is used to refer to the composition of independently executing processes.
Considering the system as a whole, and determining the independently executing processes within it and how they may be composed, it is clear that a distributed system has significant room for concurrency, which serves to aid speed and memory usage.
Assuming distinct nodes for performing operations on chunks, a remarkable thing would occur on the requesting client: nothing happens.
The processing occurs in an entirely separate memory space, with a different processor.
Were the client performing operations on regular, non-chunk objects, the processing would bind up the client, and it would block until the operation completed.
This recognition opens up a broad mix of possibilities and complications.
In terms of possibilities, the potential for chunk operations to be non-blocking to the client means that operations may be \textbf{asynchronous}, which would allow for significantly more efficient ordering of events within the system.
Long operations on chunks may take place side-by-side with client-side operations on local objects, neither interfering with each other. The converse of such a possibility is the new potential for \textbf{race conditions}, where the unordered timing of events may lead to undesirable behaviour. 

Through the defining feature of distributed data being incapable of direct reference, we have established that there must be some entity through which to indirectly interact with the distributed data.
This entity we have given the appellation of \textbf{`Distributed Object Reference'}, and serves as a proxy which is manipulated by the user.
This relationship between the reference and the distributed data is known as \textbf{``adequacy''} in formal semantics, with the role of the distributed data as the object of indirect interaction being called the \textbf{``referent''} \citep{gordon1984}.
It must be established that the reference itself may be of conceptual interest.
The referent chunks themselves, if completely transparent to a reference, possess no information on their relation to each other.
This information needs to be captured somewhere accessible to the user, with most distributed algorithms being dependent on the knowledge of how the underlying chunks relate, in dimension or quantity. 
Generalising the terminology of \citet{quine1979}, the distinction between the \textbf{use} and \textbf{mention} with respect to  distributed objects lies in the contrasting operations that may be intended of a distributed object reference: interaction may be intended for either the referent distributed object (use), or the reference itself (mention).
Such semantics of access to the reference and referent serve as the basis for much of metaprogramming, and may be considered a valuable component of a distributed statistical system.

\section{User Interface}\label{sec:ui}

\pkg{largescaler} serves as a functioning system, capable of performing complex statistical analyses over datasets spanning hundreds of nodes.
The implementation of this system makes use of a layered approach, wherein each layer targets a different category of user.
The \pkg{largescaler} interface is the principal new contribution by this project, delivering a novel means of interacting with distributed data through meaningful primitives defined at every level of abstraction.
A key offering of the layered approach is the ease by which a user of the package can traverse the levels as needed, with irrelevant information remaining hidden until required.
The levels of abstraction correspond to users of the package, given as the following:

\begin{description}
    \item[Analyst] A user solely interested in using the provided models and statistical functions in order to attain insight into some larger-than-memory data, typically a distributed data frame. All details of distribution are abstracted away.
    \item[Researcher] A user seeking to develop their own distributed statistical models. Distributed objects are to be considered as singular cohesive objects.
    \item[Developer] A user seeking greater expressivity in the definition of statistical models. Chunks are considered a relevant concern to be manipulated directly.
    \item[Architect] A user intending to directly modify the network topology of the distributed system, mainly in order to attain major efficiency gains.
\end{description}

Each of the users are served by the aforementioned packages making up the framework, in turn serving a logical layer of abstraction.
This mapping is given in Table \ref{tab:layer}.

\begin{table}[h!]
\centering
\caption{A mapping of logical layers, users and the respective packages provided by the \pkg{largescaler} framework to enable the use.}
\label{tab:layer}
\begin{tabular}{@{}lll@{}}
\toprule
Layer & User & Package \\ \midrule
Model Usage & Analyst & \pkg{largescalemodelr} \\
Model Description & Researcher & \pkg{largescaler} \\
Cluster Interaction & Developer & \pkg{chunknet} \\
Communication & Architect & \pkg{orcv} \\ \bottomrule
\end{tabular}
\end{table}

Owing to considerations of space, this description of the user interface will focus solely on the key offerings of the \pkg{largescaler} user interface, with descriptions of other components and packages having a fuller treatment in \citet{cairns2023}.
Use of the API is given by way of example below.

Consider first a simple operation of summation.
Given some vector $x$ that is broken up into $j$ chunks, yielding $j$ subvectors of length $i$.
Due to associativity the sum of the whole is the sum of the sum of the parts, as shown in Equation \ref{eqn:sum}.

\begin{equation}\label{eqn:sum}
    \sum_i x_i= \sum_{j,i}x_{ij} = \sum_j\sum_i x_{ij}
\end{equation}

In an effort to maintain as close proximity as possible between the mathematical description and the provided interface, this may be written in \pkg{largescaler} as in the following \proglang{R} code:

\code{ d(sum)(x) |> emerge() |> sum() }

Here, the \code{d()} applicator function transforms the base \code{sum()} function to work over distributed objects, in this example given by \code{x}.
\code{sum()} is therefore sent to each chunk, yielding a new distributed object as the output.
This output distributed object can be brought back to the requesting client and combined using the provided \code{emerge()} function, yielding a regular \proglang{R} numeric vector of sums.
This vector of sums may then be summed as normal, providing the final result.
The given example is actually a very simple application of map-reduce, and could effectively serve as the \code{sum()} method for distributed objects

Consider something slightly more complex: the arithmetic mean.
Again, a chunked mathematical description is given in Equation \ref{eqn:mean}

\begin{equation}\label{eqn:mean}
    \overline{x} = \frac{\sum_{i}x_{i}}{n} = \frac{\sum_{j,i}x_{ij}}{\sum_j n_j}
\end{equation}

A related means of specification through \pkg{largescaler} is possible, given in the following code:

\code{sum(x) / { d(length)(x) |> sum() } }

Here we build on the distributed sum introduced above, but the total length of The distributed object is relevent as the denominator.
Assuming a \code{sum()} method for distributed objects as described, and the math group generic defined in a similar fashion, the denominator is defined using the same \code{d()} function that sends a \code{length} computation to all of the chunks.

Finally, consider the cumulative sum.
It is important in this case to think of chunks as being in series, which is determined by the structure of the distributed object reference.
The main difference between a non-distributed and a distributed version of cumulative sum is that for each chunk in the series, computation requires the cumulative sum of the previous chunk as a starting value.
Using a chunked mathematical description, cumulative sum may be described by Equation \ref{eqn:cumsum}.

\begin{equation}
\begin{gathered}\label{eqn:cumsum}
    S_i = S_{i-1}+x_i, \quad S_0 = 0\\
    \iff S_{i,j} = S_{i-1,j} + x_{i,j}, \quad S_{0,j} = S_{n_i,j-1},\\
    \qquad S_{0,0} = 0
\end{gathered}
\end{equation}

This can be expressed in a functional manner using the \textit{reduce} operator, also known as a \textit{fold}, and the \pkg{largescaler} framework provides a distributed form of such a function, where the results of one chunk are sent as the initial value to the reduce function as applied to the next chunk and so on in series.
An example of a reduce operation is given in Figure \ref{fig:dreduce}.

\begin{figure}[ht]
\begin{center}
    \input{dreduce.tex}
\caption{Example distributed reduce pattern from controlling process.}
\label{fig:dreduce}
\end{center}
\end{figure}

This is put to use for cumulative sum by \pkg{largescaler} by the following code:

\code{ dReduce(cumsum, x) |> emerge() }

A distributed object is returned that by default just holds the final accumulation, consisting of one single chunk.

\section{A Distributed Model in largescaler}

We now move to the central problem that prompted this work; defining a novel distributed statistical algorithm.
Rather than detour with a truly novel algorithm, it is prefereable to engage with something that is familiar, but holds a fairly generic form that novel analyses often share.
Let's consider distributed LASSO regression, using the Alternating Direction Method of Multipliers, as described by \citet{mateos2010}.
We begin by assessing the mathematical form in Subsection \ref{sec:mathlasso}, followed by the ``standard R'' means of writing a chunked algorithm in contraposition with the \pkg{largescaler} manner in Subsection \ref{sec:rlasso}.

\subsection{Distributed LASSO Mathematical Definition}\label{sec:mathlasso}

This section seeks to give a brief taste of what a mathematical formulation for a distributed statistical model takes, in order to demonstrate the semantic and syntactic similarity to \pkg{largescaler} in Subsection \ref{sec:rlasso}.
A richer description, with significant background, is given in \citet{boyd2011}.

We begin with a description of the input data, as given in Equation \ref{eqn:mathlassodata}

\begin{equation}
\begin{aligned}\label{eqn:mathlassodata}
    A = \begin{bmatrix}
        A_1\\
        \vdots \\
        A_N
    \end{bmatrix},&
    \quad b=\begin{bmatrix}
        b_1\\
        \vdots \\
        b_N
    \end{bmatrix}\\
    A_i \in \mathbb{R}^{m_i\times n},& \quad b_i \in \mathbb{R}^{m_i}
\end{aligned}
\end{equation}

The starting data includes a column block matrix of explanatory variables, $A$, consisting of $N$ submatrices.
This is equivalent to a distributed object consisting of $N$ chunks.
Here, each chunk is of the standard form where rows are individual observations and columns are variables.
We also have a block matrix $b$ of the same number of chunks, with each chunk being the column vector of response variables to the corresponding $A$ chunks.

The standard form of the LASSO as an optimisation problem is expressed in Equation \ref{eqn:mathlassointention}.

\begin{equation}\label{eqn:mathlassointention}
    \begin{aligned}
        f(x) &= \frac{1}{2} \left\| A_i x_i - b_i \right\|^2_2 \\
        g(z) &= \lambda \left| z \right|_1 \\
        \text{minimise} \quad f(x) + g(z) &\quad \text{subject to} \quad x = z
    \end{aligned} 
\end{equation}

The body of the ADMM loop is given by Equation \ref{eqn:mathlassoloop}.
Of note is the complexity, the presence of iteration, and the interactions between sets of chunks getting reduced and emerged.

\begin{equation}\label{eqn:mathlassoloop}
    \begin{aligned}
        x_i^{k+1} &:= (A_i^T A_i + \rho I)^{-1}(A^T b + \rho(z^k - u_i^k))\\
        z^{k+1} &:= S_\frac{\lambda}{\rho N} (\overline{x}^{k+1} + \overline{u}^k) \\
        u_i^{k+1} &:= u_i^k + x_i^{k+1} - z^{k+1}
    \end{aligned} 
\end{equation}

\subsection{Distributed LASSO R Description}\label{sec:rlasso}

This subsection gives both base R syntax for working with a local chunked dataset, as well as the minimal changes that are required when using \pkg{largescaler} to transform the expressions to handle distributed data.
The core substance of this subsection is to demonstrate the ease with which \proglang{R} is able to meet a mathematical definition, and the successive ease by which \pkg{largescaler} is able to turn that into a truly distributed algorithm.
As in Subsection \ref{sec:mathlasso}, the working example is given of distributed LASSO.
Consider first some chunked data, given as a diff below.
In the diff, semantic differences are demarcated through underlining, with shared code given centrally, with no dividing line.
On the left of the diff we have how the LASSO as described might be encoded in the absence of the API, assuming that the data fit into memory, and on the right, we make use of large scale R with no such constraining assumption.

In the \pkg{largescaler} code, distributed data may come from multiple files and multiple hosts holding the chunks, and this is easily provided for.
The distribution comes with the necessity to carefully differentiate between the reference of the distributed object, and the distributed object itself.
The ability to explicitly distribute local values to particular locations is also demonstrated here.

\begin{table}[H]
    \centering
\begin{tabular}{p{\dimexpr 0.5\linewidth-2\tabcolsep} | p{\dimexpr 0.5\linewidth-2\tabcolsep}}
R & largescaler \\ \midrule
\begin{Verbatim}[commandchars=\#\[\]]
A <- read.#underline[csv("~/filepath/A")]

b <- read.#underline[csv("~/filepath/b")]

M_N <- dim(#underline[A])
\end{Verbatim}
&
\begin{Verbatim}[commandchars=\#\[\]]
A <- read.#underline[dmatrix(c("host1:~/filepath/A1",]
                    #underline["host2:~/filepath/A2"))]
b <- read.#underline[dmatrix(c("host1:~/filepath/b1",]
                    #underline["host2:~/filepath/b2"))]
M_N <- dim(#underline[Ref(A)])
\end{Verbatim}
\end{tabular}
\begin{Verbatim}[commandchars=\#\[\]]
                        m <- ncol(A)
                        S_z <- S(lambda/(rho*M_N[2]))
                        z_curr <- rep(1, m)
\end{Verbatim}
\begin{tabular}{p{\dimexpr 0.5\linewidth-2\tabcolsep} | p{\dimexpr 0.5\linewidth-2\tabcolsep}}
\begin{Verbatim}[commandchars=\#\[\]]
x_curr <- u_curr <- #underline[rep(list(z_curr),]
                                    #underline[N)]
\end{Verbatim}
&
\begin{Verbatim}[commandchars=\#\[\]]
x_curr <- u_curr <- #underline[distribute(z_curr,]
                               #underline[where=A)]
\end{Verbatim}
\end{tabular}
\end{table}

The layout of the data is followed by the iterative loop, given in the below diff.
Within the iterative loop, we can see that very little is actually needed to be changed in order to distribute this algorithm.
We make use of a function that operates on distributed objects which we define ourselves in the successive diff, as well as the emerge to bring the distributed local as we saw before.

\begin{table}[H]
\centering
\begin{tabular}{p{\dimexpr 0.5\linewidth-2\tabcolsep} | p{\dimexpr 0.5\linewidth-2\tabcolsep}}
R & largescaler \\ \midrule
\end{tabular}
\begin{Verbatim}[commandchars=\#\[\]]
             while (l1_norm(z_curr - z_prev) >tolerance) {
                 x_prev <- x_curr; z_prev <- z_curr; u_prev <- u_curr
\end{Verbatim}
\begin{tabular}{p{\dimexpr 0.5\linewidth-2\tabcolsep} | p{\dimexpr 0.5\linewidth-2\tabcolsep}}
\begin{Verbatim}[commandchars=\#\[\]]
x_curr <- #underline[mapply(]x.update, x_prev,
                     A, b, u_prev,
     #underline[MoreArgs = list(rho, z_prev))]
z_curr <- S_z(rowMeans(x_curr) +
              rowMeans(u_curr))
u_curr <- #underline[mapply(function(u_prev,]
                  #underline[x_curr, z_curr)]
        u_prev + x_curr - z_curr#underline[,]
                  #underline[u_prev, x_curr,]
         #underline[MoreArgs = list(z_curr))]
\end{Verbatim}
& 
\begin{Verbatim}[commandchars=\#\[\]]
x_curr <- #underline[d.]x_update(x_prev, A, b,
              u_prev, #underline[rho, z_prev])

z_curr <- S_z(rowMeans(#underline[emerge(x_curr)]) +
              rowMeans(#underline[emerge(u_curr))])
u_curr <- u_prev + x_curr - z_curr
\end{Verbatim}
\end{tabular}
\begin{Verbatim}[commandchars=\#\[\]]
                                        }
                                    z_curr
\end{Verbatim}
\end{table}

Note the significantly simplified and reduced logic in switching to \pkg{largescaler}, which bears a far closer resemblance to the mathematical description.
The \code{x\_update()} function given above is exemplary of the approach provided by \pkg{largescaler}, which allows for the switching of a local to a distributed function through the higher-order \code{d()} function, as demonstrated below:

\begin{table}[H]
\centering
\begin{tabular}{p{\dimexpr 0.5\linewidth-2\tabcolsep} | p{\dimexpr 0.5\linewidth-2\tabcolsep}}
R & largescaler \\ \midrule
\begin{Verbatim}[commandchars=\#\[\]]
#underline[x_update] <- function(x_prev, A, b,
\end{Verbatim}
& 
\begin{Verbatim}[commandchars=\#\[\]]
#underline[d.x_update] <- #underline[d(]function(x_prev, A, b,
\end{Verbatim}
\end{tabular}
\begin{Verbatim}[commandchars=\#\[\]]
                     u_prev, rho, z_prev) {
                optim(x_prev, function(x_prev)
                        (1/2)*l2_norm(A %*% x_prev - b)^2 +
                        (rho/2)*l2_norm(x_prev - z_prev + u_prev)^2)$par
\end{Verbatim}
\begin{tabular}{p{\dimexpr 0.5\linewidth-2\tabcolsep} | p{\dimexpr 0.5\linewidth-2\tabcolsep}}
\begin{Verbatim}[commandchars=\#\[\]]
                 }
\end{Verbatim}
&
\begin{Verbatim}[commandchars=\#\[\]]
                 }#underline[)]
\end{Verbatim}
\\
\end{tabular}
\end{table}

And this serves to define a distributed LASSO, using ADMM.
                    
\section{Discussion/Conclusions}

The \pkg{largescaler} system has been proven over a number of application areas.
Data manipulation is a basic necessity, as it is required for modelling, and is provided well by other systems.
\pkg{largescaler} is capable of a full set of data manipulations, including all that are provided by the \pkg{dplyr} package.
Model fitting is demonstrated in the proof-of-concept \pkg{largescalemodelr} package, which includes a variety of models, including linear models and generalised linear models.
Work is currently underway to develop examples of boosted models, as well as a convex optimisation methods such as the alternating direction method of multipliers.

The scope for future work remains significant, enabled by the high level of extensibility provided by the system.
External systems which serve to monitor performance or take up the role of garbage collection would grant the possibility of greater reliability.
Robustness could be gained through self-healing datasets, a potential that has a precedent in a current prototype, which allows for resiliance to node failure in a more efficient manner than that of the current Resilient Distributed Datasets~\citep{zaharia2012resilient}.
Further resiliance can be gained within the system through operating the location service as a distributed hash table, leaving no central point of failure in a fully peer-to-peer system.

\bibliographystyle{jds}
\bibliography{largescaler-jason-cairns}

\end{document}